{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from Statistics import Statistics\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import CuDNNLSTM, Dropout,Dense,Input,add\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, LearningRateScheduler\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras import optimizers\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SEED = 9\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP500_df = pd.read_csv('bloombergData/SPXconst.csv')\n",
    "all_companies = list(set(SP500_df.values.flatten()))\n",
    "all_companies.remove(np.nan)\n",
    "\n",
    "constituents = {'-'.join(col.split('/')[::-1]):set(SP500_df[col].dropna()) \n",
    "                for col in SP500_df.columns}\n",
    "\n",
    "constituents_train = {} \n",
    "for test_year in range(1993,2016):\n",
    "    months = [str(t)+'-0'+str(m) if m<10 else str(t)+'-'+str(m) \n",
    "              for t in range(test_year-3,test_year) for m in range(1,13)]\n",
    "    constituents_train[test_year] = [list(constituents[m]) for m in months]\n",
    "    constituents_train[test_year] = set([i for sublist in constituents_train[test_year] \n",
    "                                         for i in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeLSTM():\n",
    "    inputs = Input(shape=(240,1))\n",
    "    x = CuDNNLSTM(25,return_sequences=False)(inputs)\n",
    "    x = Dropout(0.1)(x)\n",
    "    outputs = Dense(2,activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizers.RMSprop(),\n",
    "                          metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "    \n",
    "\n",
    "def callbacks_req(model_type='LSTM'):\n",
    "    csv_logger = CSVLogger(model_folder+'/training-log-'+model_type+'-'+str(test_year)+'.csv')\n",
    "    filepath = model_folder+\"/model-\" + model_type + '-' + str(test_year) + \"-E{epoch:02d}.h5\"\n",
    "    model_checkpoint = ModelCheckpoint(filepath, monitor='val_loss',save_best_only=False, period=1)\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss',mode='min',patience=10,restore_best_weights=True)\n",
    "    return [csv_logger,earlyStopping,model_checkpoint]\n",
    "\n",
    "def reshaper(arr):\n",
    "    arr = np.array(np.split(arr,3,axis=1))\n",
    "    arr = np.swapaxes(arr,0,1)\n",
    "    arr = np.swapaxes(arr,1,2)\n",
    "    return arr\n",
    "\n",
    "def trainer(train_data,test_data,model_type='LSTM'):\n",
    "    np.random.shuffle(train_data)\n",
    "    train_x,train_y,train_ret = train_data[:,2:-2],train_data[:,-1],train_data[:,-2]\n",
    "    train_x = np.reshape(train_x,(len(train_x),240,1))\n",
    "    train_y = np.reshape(train_y,(-1, 1))\n",
    "    train_ret = np.reshape(train_ret,(-1, 1))\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc.fit(train_y)\n",
    "    enc_y = enc.transform(train_y).toarray()\n",
    "    train_ret = np.hstack((np.zeros((len(train_data),1)),train_ret)) \n",
    "\n",
    "    if model_type == 'LSTM':\n",
    "        model = makeLSTM()\n",
    "    else:\n",
    "        return\n",
    "    callbacks = callbacks_req(model_type)\n",
    "    \n",
    "    model.fit(train_x,\n",
    "              enc_y,\n",
    "              epochs=1000,\n",
    "              validation_split=0.2,\n",
    "              callbacks=callbacks,\n",
    "              batch_size=512\n",
    "              )\n",
    "\n",
    "    dates = list(set(test_data[:,0]))\n",
    "    predictions = {}\n",
    "    for day in dates:\n",
    "        test_d = test_data[test_data[:,0]==day]\n",
    "        test_d = np.reshape(test_d[:,2:-2], (len(test_d),240,1))\n",
    "        predictions[day] = model.predict(test_d)[:,1]\n",
    "    return model,predictions\n",
    "\n",
    "def trained(filename,train_data,test_data):\n",
    "    model = load_model(filename)\n",
    "\n",
    "    dates = list(set(test_data[:,0]))\n",
    "    predictions = {}\n",
    "    for day in dates:\n",
    "        test_d = test_data[test_data[:,0]==day]\n",
    "        test_d = np.reshape(test_d[:,2:-2],(len(test_d),240,1))\n",
    "        predictions[day] = model.predict(test_d)[:,1]\n",
    "    return model,predictions     \n",
    "\n",
    "def simulate(test_data,predictions):\n",
    "    rets = pd.DataFrame([],columns=['Long','Short'])\n",
    "    k = 10\n",
    "    for day in sorted(predictions.keys()):\n",
    "        preds = predictions[day]\n",
    "        test_returns = test_data[test_data[:,0]==day][:,-2]\n",
    "        top_preds = predictions[day].argsort()[-k:][::-1] \n",
    "        trans_long = test_returns[top_preds]\n",
    "        worst_preds = predictions[day].argsort()[:k][::-1] \n",
    "        trans_short = -test_returns[worst_preds]\n",
    "        rets.loc[day] = [np.mean(trans_long),np.mean(trans_short)] \n",
    "    print('Result : ',rets.mean())  \n",
    "    return rets       \n",
    "\n",
    "    \n",
    "def create_label(df_open,df_close,perc=[0.5,0.5]):\n",
    "    if not np.all(df_close.iloc[:,0]==df_open.iloc[:,0]):\n",
    "        print('Date Index issue')\n",
    "        return\n",
    "    perc = [0.]+list(np.cumsum(perc))\n",
    "    label = (df_close.iloc[:,1:]/df_open.iloc[:,1:]-1).apply(\n",
    "            lambda x: pd.qcut(x.rank(method='first'),perc,labels=False), axis=1)\n",
    "    return label[1:]\n",
    "\n",
    "def create_stock_data(df_open,df_close,st,m=240):\n",
    "    st_data = pd.DataFrame([])\n",
    "    st_data['Date'] = list(df_close['Date'])\n",
    "    st_data['Name'] = [st]*len(st_data)\n",
    "    daily_change = df_close[st]/df_open[st]-1\n",
    "    for k in range(m)[::-1]:\n",
    "        st_data['IntraR'+str(k)] = daily_change.shift(k)\n",
    "\n",
    "    st_data['IntraR-future'] = daily_change.shift(-1)    \n",
    "    st_data['label'] = list(label[st])+[np.nan] \n",
    "    st_data['Month'] = list(df_close['Date'].str[:-3])\n",
    "    st_data = st_data.dropna()\n",
    "    \n",
    "    trade_year = st_data['Month'].str[:4]\n",
    "    st_data = st_data.drop(columns=['Month'])\n",
    "    st_train_data = st_data[trade_year<str(test_year)]\n",
    "    st_test_data = st_data[trade_year==str(test_year)]\n",
    "    return np.array(st_train_data),np.array(st_test_data) \n",
    "\n",
    "def scalar_normalize(train_data,test_data):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_data[:,2:-2])\n",
    "    train_data[:,2:-2] = scaler.transform(train_data[:,2:-2])\n",
    "    test_data[:,2:-2] = scaler.transform(test_data[:,2:-2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_folder = 'models1LSTM-I'\n",
    "result_folder = 'results1LSTM-I'\n",
    "\n",
    "for test_year in range(1993,2020):\n",
    "    \n",
    "    print('-'*40)\n",
    "    print(test_year)\n",
    "    print('-'*40)\n",
    "    \n",
    "    filename = 'bloombergData/Open-'+str(test_year-3)+'.csv'\n",
    "    df_open = pd.read_csv(filename)\n",
    "    filename = 'bloombergData/Close-'+str(test_year-3)+'.csv'\n",
    "    df_close = pd.read_csv(filename)\n",
    "    \n",
    "    label = create_label(df_open,df_close)\n",
    "    stock_names = sorted(list(constituents[str(test_year-1)+'-12']))\n",
    "    train_data,test_data = [],[]\n",
    "\n",
    "    start = time.time()\n",
    "    for st in stock_names:\n",
    "        st_train_data,st_test_data = create_stock_data(df_open,df_close,st)\n",
    "        train_data.append(st_train_data)\n",
    "        test_data.append(st_test_data)\n",
    "        \n",
    "    train_data = np.concatenate([x for x in train_data])\n",
    "    test_data = np.concatenate([x for x in test_data])\n",
    "    \n",
    "    scalar_normalize(train_data,test_data)\n",
    "    print(train_data.shape,test_data.shape,time.time()-start)\n",
    "    \n",
    "    model,predictions = trainer(train_data,test_data)\n",
    "    returns = simulate(test_data,predictions)\n",
    "    returns.to_csv(result_folder+'/avg_daily_rets-'+str(test_year)+'.csv')\n",
    "    \n",
    "    result = Statistics(returns.sum(axis=1))\n",
    "    result.shortreport() \n",
    "    \n",
    "    with open(result_folder+\"/avg_returns.txt\", \"a\") as myfile:\n",
    "        res = '-'*30 + '\\n'\n",
    "        res += str(test_year) + '\\n'\n",
    "        res += 'Mean = ' + str(result.mean()) + '\\n'\n",
    "        res += 'Sharpe = '+str(result.sharpe()) + '\\n'\n",
    "        res += '-'*30 + '\\n'\n",
    "        myfile.write(res)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
